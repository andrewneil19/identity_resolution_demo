{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers scikit-learn"
      ],
      "metadata": {
        "id": "1_KhuztcMcdy"
      },
      "id": "1_KhuztcMcdy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Load an encoder-based model (BERT-based)\n",
        "# This model is trained specifically for semantic similarity\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# To access BigQuery dataset\n",
        "from google.cloud import bigquery"
      ],
      "metadata": {
        "id": "J2Zh7bYAMdsX"
      },
      "execution_count": null,
      "outputs": [],
      "id": "J2Zh7bYAMdsX"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define GCP project and select candidates data\n",
        "\n",
        "project_id = \"clean-energy-projects\"\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = \"\"\"\n",
        "SELECT * FROM `county_officials_demo.candidates`\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "lm1cWPT2OJKR"
      },
      "id": "lm1cWPT2OJKR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataframe with candidates data\n",
        "df_candidates = client.query(query).to_dataframe()"
      ],
      "metadata": {
        "id": "-xg8DWhiNhpu"
      },
      "id": "-xg8DWhiNhpu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect\n",
        "df_candidates.shape\n",
        "df_candidates.head(15)"
      ],
      "metadata": {
        "id": "gPR5e4f0OteL"
      },
      "id": "gPR5e4f0OteL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vector embeddings of names for both a and b sides\n",
        "embeddings_a = model.encode(df_candidates['name_a'].tolist())\n",
        "embeddings_b = model.encode(df_candidates['name_b'].tolist())"
      ],
      "metadata": {
        "id": "eBwEdeNiNLld"
      },
      "id": "eBwEdeNiNLld",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate similarity for each pair of names\n",
        "similarities = []\n",
        "for i in range(len(embeddings_a)):\n",
        "    sim = cosine_similarity([embeddings_a[i]], [embeddings_b[i]])[0][0]\n",
        "    similarities.append(sim)\n",
        "\n",
        "df_candidates['similarity_score'] = similarities\n",
        "\n",
        "print(f\"  Mean: {np.mean(similarities):.3f}\")\n",
        "print(f\"  Min: {np.min(similarities):.3f}\")\n",
        "print(f\"  Max: {np.max(similarities):.3f}\")"
      ],
      "metadata": {
        "id": "cY9RdzUuN7j_"
      },
      "id": "cY9RdzUuN7j_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add ground truth column\n",
        "df_candidates['is_true_match'] = (df_candidates['true_id_a'] == df_candidates['true_id_b']).astype(int)\n",
        "\n",
        "# Compare scores for matches vs non-matches\n",
        "true_matches = df_candidates[df_candidates['is_true_match'] == 1]\n",
        "false_matches = df_candidates[df_candidates['is_true_match'] == 0]\n",
        "\n",
        "print(f\"TRUE MATCHES (same person):\")\n",
        "print(f\"  Count: {len(true_matches)}\")\n",
        "print(f\"  Mean similarity: {true_matches['similarity_score'].mean():.3f}\")\n",
        "print(f\"  Min similarity: {true_matches['similarity_score'].min():.3f}\")\n",
        "print(f\"\\nFALSE MATCHES (different people):\")\n",
        "print(f\"  Count: {len(false_matches)}\")\n",
        "print(f\"  Mean similarity: {false_matches['similarity_score'].mean():.3f}\")\n",
        "print(f\"  Max similarity: {false_matches['similarity_score'].max():.3f}\")"
      ],
      "metadata": {
        "id": "Po3PIz1gQgrd"
      },
      "id": "Po3PIz1gQgrd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set multiple similarity thresholds\n",
        "# Goal is to find best similarity threshold\n",
        "thresholds = np.arange(0.2, 0.8, 0.05)\n",
        "results = []\n",
        "\n",
        "for threshold in thresholds:\n",
        "    # Predict matches\n",
        "    df_candidates['predicted_match'] = (df_candidates['similarity_score'] >= threshold).astype(int)\n",
        "\n",
        "    # Find true positives (tp), false positives (fp), true negatives (tn),\n",
        "    # and false negatives (fn)\n",
        "    tp = ((df_candidates['predicted_match'] == 1) & (df_candidates['is_true_match'] == 1)).sum()\n",
        "    fp = ((df_candidates['predicted_match'] == 1) & (df_candidates['is_true_match'] == 0)).sum()\n",
        "    tn = ((df_candidates['predicted_match'] == 0) & (df_candidates['is_true_match'] == 0)).sum()\n",
        "    fn = ((df_candidates['predicted_match'] == 0) & (df_candidates['is_true_match'] == 1)).sum()\n",
        "\n",
        "    # Calculate classic evaluation metrics\n",
        "\n",
        "    accuracy = (tp + tn) / len(df_candidates)\n",
        "\n",
        "    # Precision\n",
        "    if (tp + fp) > 0:\n",
        "      precision = tp / (tp + fp)\n",
        "    else:\n",
        "      precision = 0\n",
        "\n",
        "    # Recall\n",
        "    if (tp + fn) > 0:\n",
        "      recall = tp / (tp + fn)\n",
        "    else:\n",
        "      recall = 0\n",
        "\n",
        "    # F1 Score\n",
        "    if (precision + recall) > 0:\n",
        "      f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    else:\n",
        "      f1 = 0\n",
        "\n",
        "    results.append({\n",
        "        'threshold': threshold,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    })\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "print(df_results.sort_values('f1', ascending=False).head())"
      ],
      "metadata": {
        "id": "PXQB7NtMRy_D"
      },
      "id": "PXQB7NtMRy_D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply best threshold - 0.55\n",
        "BEST_THRESHOLD = 0.55\n",
        "df_candidates['predicted_match'] = (df_candidates['similarity_score'] >= BEST_THRESHOLD).astype(int)\n",
        "\n",
        "# Calculate final metrics\n",
        "tp = ((df_candidates['predicted_match'] == 1) & (df_candidates['is_true_match'] == 1)).sum()\n",
        "fp = ((df_candidates['predicted_match'] == 1) & (df_candidates['is_true_match'] == 0)).sum()\n",
        "tn = ((df_candidates['predicted_match'] == 0) & (df_candidates['is_true_match'] == 0)).sum()\n",
        "fn = ((df_candidates['predicted_match'] == 0) & (df_candidates['is_true_match'] == 1)).sum()\n",
        "\n",
        "print(f\"FINAL RESULTS (threshold = {BEST_THRESHOLD}):\")\n",
        "print(f\"  True Positives: {tp}\")\n",
        "print(f\"  False Positives: {fp}\")\n",
        "print(f\"  True Negatives: {tn}\")\n",
        "print(f\"  False Negatives: {fn}\")\n",
        "print(f\"\\n  Precision: {tp/(tp+fp):.1%}\")\n",
        "print(f\"  Recall: {tp/(tp+fn):.1%}\")\n",
        "print(f\"  Accuracy: {(tp+tn)/len(df_candidates):.1%}\")\n",
        "\n",
        "# Show example matches\n",
        "print(\"\\n=== EXAMPLE CORRECT MATCHES ===\")\n",
        "correct_matches = df_candidates[(df_candidates['predicted_match'] == 1) & (df_candidates['is_true_match'] == 1)]\n",
        "print(correct_matches[['name_a', 'name_b', 'similarity_score']].head(5))\n",
        "\n",
        "# Show missed matches (false negatives)\n",
        "print(\"\\n=== MISSED MATCHES (False Negatives) ===\")\n",
        "missed = df_candidates[(df_candidates['predicted_match'] == 0) & (df_candidates['is_true_match'] == 1)]\n",
        "print(missed[['name_a', 'name_b', 'similarity_score', 'true_id_a']])"
      ],
      "metadata": {
        "id": "Q7Y9TiIGaOeX"
      },
      "id": "Q7Y9TiIGaOeX",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "import_data_and_identity_resolution"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}